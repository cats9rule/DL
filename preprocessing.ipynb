{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _correctTypos(text: str) -> str:\n",
    "    blob = TextBlob(text).correct()\n",
    "    return str(blob)\n",
    " \n",
    "def _cleanLinks(text):\n",
    "    \"\"\"Cleans links and HTML tags from text.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def _cleanAbbreviations(text):\n",
    "    text = re.sub(r\"i['\\s]?m\\s\", \"i am \", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"\\su['\\s]r\\s\", \" you are \", text)\n",
    "    text = re.sub(r\"he['\\s]?s\\s\", \"he is \", text)\n",
    "    text = re.sub(r\"she['\\s]?s\", \"she is\", text)\n",
    "    text = re.sub(r\"that['\\s]?s\", \"that is\", text)\n",
    "    text = re.sub(r\"what['\\s]?s\", \"what is\", text)\n",
    "    text = re.sub(r\"where['\\s]?s\", \"where is\", text)\n",
    "    text = re.sub(r\"[\\'\\s]ll\\s\", \" will \", text)\n",
    "    text = re.sub(r\"[\\'\\s]ll\\s\", \" will \", text)\n",
    "    text = re.sub(r\"[\\'\\s]ve\\s\", \" have \", text)\n",
    "    text = re.sub(r\"[\\'\\s]re\\s\", \" are \", text)\n",
    "    text = re.sub(r\"[\\'\\s]d\\s\", \" would \", text)\n",
    "    text = re.sub(r\"won['\\s]?t\", \"will not\", text)\n",
    "    text = re.sub(r\"don['\\s]?t\", \"do not\", text)\n",
    "    text = re.sub(r\"didn['\\s]?t\", \"did not\", text)\n",
    "    text = re.sub(r\"can['\\s]?t\", \"can not\", text)\n",
    "    text = re.sub(r\"couldn['\\s]?t\", \"could not\", text)\n",
    "    text = re.sub(r\"haven['\\s]?t\", \"have not\", text)\n",
    "    text = re.sub(r\"\\sw(?:\\s|$)\", \" with \", text)\n",
    "    text = re.sub(r\"\\stbh\\s\", \" to be honest \", text)\n",
    "    #TODO: add more\n",
    "    return text\n",
    "\n",
    "def _removeSpecialChars(text):\n",
    "    text = re.sub(r\"[@#$%^&*(){}/;`~<>+=-]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def _removeNumbers(text: str) -> str:\n",
    "    return re.sub(r\"\\d+\", '', text)\n",
    "\n",
    "def _removePunctuation(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess(inputData: dict, fns: list) -> tuple:\n",
    "    inputData['sentiment'] = inputData['sentiment'].replace(\n",
    "    {'joy': 0, 'anger': 1, 'love': 2, 'sadness': 3, 'fear': 4, 'surprise': 5})\n",
    "    yinput = to_categorical(inputData['sentiment'].values)\n",
    "    inputData['text'] = inputData['text'].map(lambda t: normalization(t, fns))\n",
    "    xinput = inputData['text'].values\n",
    "    return (xinput, yinput)\n",
    "\n",
    "def makeTokenizer(xtrain: list):\n",
    "    tokenizer = Tokenizer(15212, oov_token='UNK')\n",
    "    tokenizer.fit_on_texts(xtrain)\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with io.open('preprocessing/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    return tokenizer\n",
    "    \n",
    "def textToSequences(text: list):\n",
    "    tokenizer = None\n",
    "    with open('preprocessing/tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "        text = pad_sequences(tokenizer.texts_to_sequences(text), maxlen=80, padding='post')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def _removeStopWords(text: str) -> str:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _lemmatization(text: str) -> str:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [lemmatizer.lemmatize(y) for y in word_tokens]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(text: str, functions: list) -> str:\n",
    "    for f in functions:\n",
    "        text = f(text)\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
